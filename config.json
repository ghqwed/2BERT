{
  // 词汇表大小，表示模型能识别的不同token数量
  "vocab_size": 30522,
  
  // 隐藏层维度，决定每个token表示的向量大小
  "hidden_size": 768,
  
  // Transformer编码器的层数，决定模型深度
  "num_hidden_layers": 12,
  
  // 注意力头的数量，用于多头注意力机制
  "num_attention_heads": 12,
  
  // 前馈神经网络中间层的维度
  "intermediate_size": 3072,
  
  // 激活函数类型，GELU是BERT使用的特殊激活函数
  "hidden_act": "gelu",
  
  // 隐藏层的dropout概率，防止过拟合
  "hidden_dropout_prob": 0.1,
  
  // 注意力权重的dropout概率
  "attention_probs_dropout_prob": 0.1,
  
  // 最大位置编码长度，决定模型能处理的最大序列长度
  "max_position_embeddings": 512,
  
  // 句子类型数量(如单句/句对)
  "type_vocab_size": 2,
  
  // 参数初始化的范围
  "initializer_range": 0.02
}
